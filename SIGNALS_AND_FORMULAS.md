# Signal Processing & Formulas - Deep Dive Documentation

## ğŸ“Š Table of Contents
1. [Overview](#overview)
2. [Core Signal Metrics](#core-signal-metrics)
3. [Mathematical Formulas](#mathematical-formulas)
4. [Signal Processing Pipeline](#signal-processing-pipeline)
5. [Filter Design](#filter-design)
6. [Decision Thresholds](#decision-thresholds)
7. [Benefits Analysis](#benefits-analysis)
8. [Advanced Techniques](#advanced-techniques)
9. [Performance Optimization](#performance-optimization)
10. [Practical Implementation Guide](#practical-implementation-guide)

---

## ğŸ¯ Overview

This project uses a sophisticated signal processing approach to detect driver drowsiness. Instead of making decisions based on individual frame measurements, it processes streams of data using mathematical filters and algorithms to:

- **Remove noise** from video artifacts and lighting changes
- **Detect patterns** that indicate drowsiness over time
- **Prevent false alarms** through hysteresis and sustained threshold checks
- **Track history** to understand trends rather than snapshots

**Key Philosophy:** *One blinking frame doesn't mean drowsiness. A pattern of sustained eye closure does.*

---

## ğŸ“ˆ Core Signal Metrics

### 1. Eye Aspect Ratio (EAR)

**What It Measures:** How open or closed the eyes are, based on facial geometry.

**Formula:**
$$\text{EAR} = \frac{||p_2 - p_6|| + ||p_3 - p_5||}{2 \times ||p_1 - p_4||}$$

Where:
- $p_1, p_4$ = Eye corners (left-right)
- $p_2, p_6, p_3, p_5$ = Eyelid points (top-bottom)
- $||...||$ = Euclidean distance

**Geometric Meaning:**
```
       p2 _____ p3
        /       \
       /         \     (vertical distances: p2-p6, p3-p5)
p1 ___           ___ p4  (horizontal distance: p1-p4)
   \             /
    \_____p5____/
      p6

EAR = (|p2-p6| + |p3-p5|) / (2 Ã— |p1-p4|)
```

**Typical Values:**
- Open eyes: **0.25 - 0.35**
- Squinting: **0.15 - 0.25**
- Closed eyes: **< 0.15**

**Why This Formula?**
1. **Rotation invariant:** Works regardless of head tilt
2. **Scale independent:** Works for different face sizes
3. **Biological basis:** Directly measures eye openness
4. **Fast computation:** Only 6 landmark points needed

**Code Implementation:**
```python
def calculate_eye_aspect_ratio(leftEye, rightEye):
    # Left eye
    earLeft = (LA.norm(leftEye[13] - leftEye[3]) + 
               LA.norm(leftEye[11] - leftEye[5])) / \
              (2 * LA.norm(leftEye[0] - leftEye[8]))
    
    # Right eye
    earRight = (LA.norm(rightEye[13] - rightEye[3]) + 
                LA.norm(rightEye[11] - rightEye[5])) / \
               (2 * LA.norm(rightEye[0] - rightEye[8]))
    
    # Average
    return (earLeft + earRight) / 2
```

---

### 2. Mouth Aspect Ratio (MAR)

**What It Measures:** How open the mouth is (yawning detection).

**Formula:**
$$\text{MAR} = \frac{||u_{14} - l_{17}|| + ||u_{12} - l_{14}||}{||u_0 - u_8|| + ||l_{12} - l_{10}||}$$

Where:
- $u_i$ = Upper lip points
- $l_i$ = Lower lip points

**Typical Values:**
- Closed mouth: **0.1 - 0.3**
- Talking: **0.4 - 0.6**
- Yawning: **> 0.7**

**Why This Formula?**
1. **Normalized:** Divides vertical by horizontal for scale independence
2. **Hysteresis friendly:** Distinct states (closed/talking/yawning)
3. **Detects fatigue signs:** Yawning is precursor to drowsiness

**Yawning Detection Logic:**
```python
if mar > 0.7:  # marThresh
    state = "Yawning"
    signal = FATIGUE_WARNING
elif mar >= 0.15:  # marThresh2
    state = "Talking"
    signal = NORMAL
else:
    state = "Closed"
    signal = NORMAL
```

---

### 3. PERCLOS (PERcentage of eye CLOSure)

**What It Measures:** Percentage of time eyes are closed in a time window.

**Formula:**
$$\text{PERCLOS} = \frac{\text{Number of frames with EAR} < \theta}{\text{Total frames in window}} \times 100\%$$

Where:
- $\theta$ = EAR threshold (typically 0.20)
- Window = typically 3 seconds (90 frames at 30 FPS)

**Calculation Steps:**

1. **Collect EAR samples:**
   ```
   Time (ms):  0    33   67   100  133  167  200  233...
   EAR:      0.28 0.27 0.15 0.10 0.08 0.12 0.20 0.25...
   Closed?:    N    N    Y    Y    Y    Y    N    N...
   ```

2. **Count closed frames in window:**
   ```
   Window (last 90 samples): [0.28, 0.27, ..., 0.10, 0.08, ...]
   Count(EAR < 0.20) = 45 frames
   ```

3. **Calculate percentage:**
   ```
   PERCLOS = (45 / 90) Ã— 100% = 50%
   ```

**Interpretation:**
- **0-20%:** Normal (occasional blinking)
- **20-50%:** Mild fatigue
- **> 50%:** Significant drowsiness (DANGER)

**Code Implementation:**
```python
class PERCLOSCalculator:
    def __init__(self, window_size=90, threshold=0.20):
        self.window_size = window_size  # 3 seconds
        self.threshold = threshold       # EAR threshold
    
    def compute(self, ear_signal):
        """
        ear_signal: numpy array of EAR values
        returns: PERCLOS percentage (0-100%)
        """
        if len(ear_signal) < self.window_size:
            window = ear_signal
        else:
            window = ear_signal[-self.window_size:]
        
        closed_count = np.sum(window < self.threshold)
        perclos = (closed_count / len(window)) * 100.0
        return perclos
```

**Why PERCLOS Over Raw EAR?**
1. **Temporal integration:** Captures sustained eye closure, not blinking
2. **Research validated:** Published studies show PERCLOS > 50% correlates with drowsiness
3. **Natural:** Matches how humans perceive fatigue (accumulated effect)
4. **Robust:** Handles blinks and temporary artifacts

---

### 4. Head Pose Angles (Roll, Pitch, Yaw)

**What They Measure:** 3D head orientation indicating head position and attention.

**The Euler Angle Decomposition:**

From rotation vector $\mathbf{r}$ estimated by solvePnP:

$$\mathbf{R} = \text{Rodrigues}(\mathbf{r})$$

Then perform RQ decomposition:

$$\mathbf{R} = \mathbf{RQ} = \mathbf{R}_z(\text{yaw}) \times \mathbf{R}_y(\text{pitch}) \times \mathbf{R}_x(\text{roll})$$

**Angle Meanings:**

```
    Y (up)
    |
    +------ X (right)
   /
  Z (forward)


Roll:  Rotation around Z-axis (tilt head left/right) [degrees]
       Positive = right ear down, Negative = left ear down
       
Pitch: Rotation around X-axis (head up/down) [degrees]
       Positive = looking up, Negative = looking down (nodding)
       
Yaw:   Rotation around Y-axis (head left/right) [degrees]
       Positive = looking right, Negative = looking left
```

**Typical Drowsy Patterns:**
- **Forward-facing:** roll â‰ˆ 0Â°, pitch â‰ˆ 0Â°
- **Nodding (drowsy):** pitch < -10Â° (looking down)
- **Distracted:** |yaw| > 25Â° (looking away)
- **Head tilt:** |roll| > 20Â° (ear toward shoulder)

**Code Implementation:**
```python
def calculate_angles(self):
    """Convert rotation vector to Euler angles"""
    rmat = cv2.Rodrigues(self.rvec)[0]
    angles = cv2.RQDecomp3x3(rmat)[0]
    
    self.roll = angles[0] * 360    # Degrees
    self.pitch = angles[1] * 360   # Degrees
    self.yaw = angles[2] * 360     # Degrees
    
    return self.roll, self.pitch, self.yaw
```

**Head-Down Signal for Drowsiness:**
```python
# During drowsiness, head pitch increases (looking down)
# Create a "head down" signal by comparing to baseline
head_down_signal = max(0, baseline_roll - current_roll)

# If head is significantly down from baseline: danger
if head_down_signal > 6.0:  # headThresh
    DANGER
```

---

### 5. Gaze Estimation

**What It Measures:** Where the driver is looking (forward vs. away).

**Formula:**
$$\text{Gaze} = \frac{||I_{\text{iris}} - C_{\text{eye}}||_{\text{left}} + ||I_{\text{iris}} - C_{\text{eye}}||_{\text{right}}}{2}$$

Where:
- $I_{\text{iris}}$ = Iris center
- $C_{\text{eye}}$ = Eye center
- $||...||$ = Euclidean distance in pixels

**Interpretation:**
- **Low gaze (< 5 pixels):** Looking straight ahead
- **Medium gaze (5-10 pixels):** Looking slightly away
- **High gaze (> 10 pixels):** Looking significantly away (distracted)

**Code Implementation:**
```python
def estimate_gaze(self, leftEye, rightEye, leftIris, rightIris):
    # Left eye center
    xLeft = (leftEye[8][0] + leftEye[0][0]) / 2
    yLeft = (leftEye[12][1] + leftEye[4][1]) / 2
    centerLeft = np.array([xLeft, yLeft], dtype=np.int32)
    
    # Right eye center
    xRight = (rightEye[8][0] + rightEye[0][0]) / 2
    yRight = (rightEye[12][1] + rightEye[4][1]) / 2
    centerRight = np.array([xRight, yRight], dtype=np.int32)
    
    # Iris centers
    lirisCenter = np.array([leftIris[0], leftIris[1]], dtype=np.int32)
    ririsCenter = np.array([rightIris[0], rightIris[1]], dtype=np.int32)
    
    # Distance from iris to eye center
    distLeft = LA.norm(lirisCenter - centerLeft)
    distRight = LA.norm(ririsCenter - centerRight)
    
    gazeAvg = (distLeft + distRight) / 2
    return gazeAvg
```

---

## ğŸ”¬ Mathematical Formulas

### Signal Filtering: Moving Average

**What It Does:** Smooths noisy signals while preserving important trends.

**Formula:**
$$y[n] = \frac{1}{M} \sum_{k=0}^{M-1} x[n-k]$$

Where:
- $x[n]$ = Input signal
- $y[n]$ = Output signal
- $M$ = Window size

**Graphical Example:**
```
Raw Signal:     0.28, 0.25, 0.12, 0.09, 0.10, 0.25, 0.28
Noise:          ^     ^     ^     ^     ^     ^     ^
                blink                           blink

After MA (M=3):
y[0] = (0.28 + 0 + 0) / 3 = 0.093
y[1] = (0.28 + 0.25 + 0) / 3 = 0.176
y[2] = (0.28 + 0.25 + 0.12) / 3 = 0.217  â† Smoothed
y[3] = (0.25 + 0.12 + 0.09) / 3 = 0.153  â† Trend visible
y[4] = (0.12 + 0.09 + 0.10) / 3 = 0.103
y[5] = (0.09 + 0.10 + 0.25) / 3 = 0.147
y[6] = (0.10 + 0.25 + 0.28) / 3 = 0.210
```

**Implementation (Convolution):**
```python
class MovingAverageFilter:
    def __init__(self, window_size=7):
        self.window_size = window_size
        # Rectangular kernel normalized to 1
        self.kernel = np.ones(window_size) / window_size
    
    def apply(self, signal):
        """Apply moving average via convolution"""
        if len(signal) < self.window_size:
            return signal
        
        # np.convolve with 'same' mode keeps output same length
        smoothed = np.convolve(signal, self.kernel, mode='same')
        return smoothed
```

**Mathematical Interpretation:**
- **Convolution:** $y[n] = (x * h)[n] = \sum x[k]h[n-k]$
- **Frequency domain:** Attenuates high frequencies
- **Time domain:** Averages neighboring samples

**Window Sizes Used:**
| Signal | Window | Duration | Purpose |
|--------|--------|----------|---------|
| EAR | 7 | 0.23 sec | Remove blinks |
| Pitch | 15 | 0.50 sec | Smooth head motion |
| MAR | 10 | 0.33 sec | Smooth mouth changes |

**Why This Filter?**
1. **Simplicity:** Single parameter to tune
2. **Causality:** Uses past data only (real-time friendly)
3. **Stability:** Always converges
4. **Interpretability:** Average of last M samples

---

### Hysteresis Control

**What It Does:** Prevents flickering between states by requiring different thresholds for transitions.

**Mathematical Definition:**

$$\text{State transitions use different thresholds:}$$

$$\text{Enter DANGER: } \text{EAR} < \theta_{\text{danger}} = 0.11$$
$$\text{Exit DANGER: } \text{EAR} > \theta_{\text{danger}} + \delta = 0.16$$

Where $\delta$ is the hysteresis margin.

**Graphical Representation:**
```
Hysteresis Loop:

EAR
 |
 â”œâ”€ 0.16 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â† Exit threshold
 |                    â”‚
 â”œâ”€ 0.13 â”€â”€â”€â”€â”€â”       â”‚
 |            â”‚       â”‚
 â”œâ”€ 0.11 â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
 |        â†‘       â†“
 â”‚    (Entry)  (Exit)
 â”œâ”€ 0.00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     Time

Effect:
- Once EAR drops below 0.11, state = DANGER
- Must rise above 0.16 (margin) to exit DANGER
- Prevents rapid fluttering at boundary
```

**Implementation:**
```python
class DrowsinessDecisionEngine:
    def __init__(self):
        # Thresholds
        self.ear_danger_threshold = 0.11
        
        # Hysteresis margins
        self.ear_hysteresis = 0.05
    
    def evaluate(self, smoothed_ear, perclos, ...):
        if self.state == 'OK':
            if smoothed_ear < self.ear_danger_threshold:
                self.state = 'DANGER'
                
        elif self.state == 'DANGER':
            # Require improvement beyond margin to exit
            if smoothed_ear > (self.ear_danger_threshold + 
                              self.ear_hysteresis):
                self.state = 'OK'
        
        return self.state, reason
```

**Why Hysteresis?**
1. **Robustness:** Handles measurement noise
2. **User experience:** Smooth state transitions
3. **Safety:** Keeps driver alert longer (conservative)
4. **Physics:** Mimics real-world systems with friction

---

### Sustained Duration Checks

**What They Do:** Require conditions to persist for minimum time before triggering alert.

**Formula:**

$$\text{Trigger Alert if: } \frac{1}{N}\sum_{i=0}^{N-1} x[i] < \theta \text{ and } N \geq N_{\text{min}}$$

Where:
- $x[i]$ = Signal values
- $\theta$ = Threshold
- $N_{\text{min}}$ = Minimum required duration (frames)

**Example: Eyes Closed Detection**

```
Threshold EAR: 0.11
Minimum sustained frames: 90 (3 seconds at 30 FPS)

Frame:  1   2   3   4   5   6   7   8   9  10  ...  90
EAR:  0.25 0.10 0.09 0.08 0.10 0.12 0.11 0.09 0.10 0.09 ...
Closed: N    Y    Y    Y    Y    Y    Y    Y    Y    Y  ...
Count:  0    1    2    3    4    5    6    7    8    9  ...

At frame 90: Count >= 90 â†’ DANGER (sustained eye closure detected)
At frame 45: Count = 45 < 90 â†’ Wait (could be normal blink)
```

**Implementation:**
```python
def evaluate(self, smoothed_ear, ...):
    # Track values
    self.last_ear_values.append(smoothed_ear)
    
    # Check sustained eye closure
    if len(self.last_ear_values) >= self.ear_sustained_frames:
        recent_ear = self.last_ear_values[-self.ear_sustained_frames:]
        mean_ear = np.mean(recent_ear)
        
        if mean_ear < self.ear_danger_threshold:
            # Eyes closed for 3+ seconds
            self.state = 'DANGER'
```

**Sustained Duration Settings:**
```python
ear_sustained_frames = 90      # 3 seconds
head_sustained_frames = 45     # 1.5 seconds
mar_sustained_frames = 45      # 1.5 seconds
```

**Why Sustained Checks?**
1. **Blink tolerance:** Normal blinks (0.1-0.3 sec) don't trigger
2. **Biological accuracy:** Drowsiness is sustained, not instantaneous
3. **Safety margin:** Gives driver multiple chances
4. **Noise rejection:** Temporary artifacts ignored

---

## ğŸ”„ Signal Processing Pipeline

### Complete Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. RAW MEASUREMENTS (Per Frame)         â”‚
â”‚   - EAR_left, EAR_right                 â”‚
â”‚   - Head angles (roll, pitch, yaw)      â”‚
â”‚   - MAR (mouth)                         â”‚
â”‚   - Gaze distance                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. SIGNAL BUFFER (Storage)              â”‚
â”‚   - Circular buffer (300 samples)       â”‚
â”‚   - 10 seconds of history @ 30 FPS      â”‚
â”‚   - Enables sliding window operations   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SIGNAL FILTERING (Noise Removal)     â”‚
â”‚   - EAR filter: M=7 (0.23 sec)          â”‚
â”‚   - Pitch filter: M=15 (0.5 sec)        â”‚
â”‚   - MAR filter: M=10 (0.33 sec)         â”‚
â”‚   - Method: Moving average convolution  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. FEATURE EXTRACTION                   â”‚
â”‚   - PERCLOS calculation (3-sec window)  â”‚
â”‚   - Head position deviation              â”‚
â”‚   - Baseline calibration                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. DECISION ENGINE                      â”‚
â”‚   - Rule-based logic with hysteresis    â”‚
â”‚   - Sustained threshold checks          â”‚
â”‚   - 3-state system (OK/WARNING/DANGER)  â”‚
â”‚   - Optional: DL model integration      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ALERT GENERATION                     â”‚
â”‚   - Visual: Frame borders (RGB)         â”‚
â”‚   - Audio: Continuous beep (800 Hz)     â”‚
â”‚   - Logged: Message with reason         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Processing Sequence Per Frame (33 ms @ 30 FPS)

**Timeline:**
```
T=0ms      T=5ms          T=15ms              T=25ms        T=33ms
â”‚          â”‚              â”‚                   â”‚             â”‚
â”œâ”€Captureâ”€â”€â”¬â”€Face Meshâ”€â”€â”€â”€â”¬â”€Extract Featuresâ”€â”€â”¬â”€Add Bufferâ”€â”€â”¬â”€Decision
â”‚ Frame    â”‚ Detection    â”‚ (EAR,MAR,Head)    â”‚ Processing  â”‚ Making
â”‚ (BGR)    â”‚ (468 pts)    â”‚ Formulas          â”‚ Filtering   â”‚ & Alert
â”‚          â”‚              â”‚                   â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€Display
```

---

## ğŸ›ï¸ Filter Design

### Why Multiple Filters with Different Window Sizes?

**Signal Characteristics:**

| Signal | Noise Type | Frequency | Window | Reason |
|--------|-----------|-----------|--------|--------|
| **EAR** | Blinks (0.1-0.3 sec) | High | 7 | Remove blinks, keep drowsiness |
| **Pitch** | Head jitter | Low | 15 | Smooth jerky head motion |
| **MAR** | Micro-yawns | Low | 10 | Detect true yawns not twitches |

**Frequency Domain Analysis:**

```
Raw EAR Signal (Frequency Domain):
â”‚         â”Œâ”€ High freq (blinks ~3 Hz)
â”‚    â”Œâ”€â”€â”€â”€â”¤
â”‚ â”Œâ”€â”€â”¤ â”Œâ”€ Low freq (drowsiness ~0.1 Hz)
â”‚ â”‚  â”‚ â”‚
â””â”€â”´â”€â”€â”´â”€â”´â”€â”€â”€ Frequency
0     3     5 Hz

After MA Filter (M=7):
Removes: High frequencies > 1 Hz
Keeps: Frequencies < 0.5 Hz (true drowsiness)
```

**Mathematical Justification:**

For moving average with window M=7:
- Cutoff frequency: $f_c = \frac{1}{M} \approx 0.14$ cycles/sample
- At 30 FPS: $0.14 \times 30 = 4.2$ Hz cutoff
- Attenuates blinks (~3 Hz) by ~70%
- Preserves drowsiness (~0.1 Hz) by ~95%

---

## ğŸ“Š Decision Thresholds

### Threshold Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRIC    â”‚ STATE    â”‚ THRESHOLD â”‚ SUSTAINED â”‚ HYSTERESIS â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ EAR       â”‚ DANGER   â”‚ 0.11      â”‚ 90 fr     â”‚ 0.05       â”‚
â”‚ PERCLOS   â”‚ DANGER   â”‚ 50%       â”‚ -         â”‚ 15%        â”‚
â”‚ Pitch Dev â”‚ DANGER   â”‚ 6 deg     â”‚ 45 fr     â”‚ 5 deg      â”‚
â”‚ MAR       â”‚ WARNING  â”‚ 0.6       â”‚ 45 fr     â”‚ 0.1        â”‚
â”‚ Yaw Dev   â”‚ WARNING  â”‚ 10 deg    â”‚ -         â”‚ -          â”‚
â”‚ Roll Dev  â”‚ DANGER   â”‚ 6 deg     â”‚ -         â”‚ -          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Threshold Calibration Process

**Baseline Establishment (First 3 seconds):**

```python
# Collect 90 frames of measurements
measurements = [rollâ‚, rollâ‚‚, ..., rollâ‚‰â‚€]

# Calculate median (robust to outliers)
baseline_roll = median(measurements)

# Deviation from baseline
roll_deviation = abs(current_roll - baseline_roll)

# Trigger if deviation exceeds threshold
if roll_deviation > 6.0:  # degrees
    DANGER
```

**Why Median Over Mean?**
- **Robustness:** One wild measurement doesn't affect baseline
- **Biological:** Represents typical posture, not average
- **Mathematical:** Better for skewed distributions

---

## ğŸ’¡ Benefits Analysis

### 1. **Noise Rejection**

**Problem:** Raw sensor data contains noise
```
Raw EAR: 0.28, 0.27, 0.26, 0.09, 0.10, 0.27, 0.29, 0.28, 0.27
                                   â†‘
                            Blink artifact
```

**Solution:** Moving average filtering
```
Filtered:  0.27, 0.27, 0.21, 0.15, 0.17, 0.21, 0.28, 0.28, 0.27
                                â†“
                         Blink smoothed away
```

**Benefit:** Fewer false alarms from temporary artifacts

---

### 2. **Temporal Integration**

**Problem:** Single-frame decisions miss sustained events
```
Frame 1: EAR = 0.09 (might be blink) â†’ Alert? âŒ Too sensitive
```

**Solution:** PERCLOS and sustained checks
```
Frames 1-90: 45 frames with EAR < 0.20
PERCLOS = 50% â†’ Alert âœ… Confirmed drowsiness
```

**Benefit:** Detects patterns, not isolated events

---

### 3. **Adaptive Thresholds**

**Problem:** Fixed thresholds don't work for all drivers
```
Driver A (wide eyes): Normal EAR = 0.35
Driver B (small eyes): Normal EAR = 0.20
Fixed threshold: 0.25 â†’ False alarms for A, missed for B
```

**Solution:** Baseline calibration
```
Driver A: baseline = 0.35 â†’ danger = 0.28
Driver B: baseline = 0.20 â†’ danger = 0.13
Personalized thresholds âœ…
```

**Benefit:** Works for all drivers without tuning

---

### 4. **Hysteresis Stability**

**Problem:** State flickering at threshold boundary
```
EAR:    0.11, 0.12, 0.11, 0.12, 0.11...
State:  D, OK, D, OK, D... (fluttering)
â†’ Audio beeping on/off continuously (annoying)
```

**Solution:** Hysteresis control
```
EAR:    0.11, 0.12, 0.11, 0.12, 0.11...
State:  D, D, D, D, D... (stable)
Enter: EAR < 0.11
Exit: EAR > 0.16 (margin = 0.05)
```

**Benefit:** Stable state transitions, better UX

---

### 5. **Multi-Signal Fusion**

**Problem:** Single signal can be misleading
```
Only EAR: 0.09 â†’ DANGER (but eyes closed due to squinting)
Only MAR: 0.3 â†’ Normal (mouth closed normally)
```

**Solution:** Combine multiple signals
```
EAR + MAR + Head pose:
Eyes closed (EAR 0.09) + No yawning (MAR 0.3) + Head normal
â†’ Likely blinking, not drowsy â†’ Don't alert
```

**Benefit:** Robust decisions from multiple perspectives

---

### 6. **Quantifiable Metrics**

**Problem:** Hard to explain why system alerted
```
"Driver looks tired" â†’ Subjective, non-actionable
```

**Solution:** Numerical metrics
```
PERCLOS: 65% (eyes 65% closed)
EAR: 0.08 (very low)
Pitch: -8Â° (head down)
Reason: "High eye closure (65% > 50%)"
â†’ Objective, traceable, explainable
```

**Benefit:** Transparency and debugging capability

---

## ğŸš€ Advanced Techniques

### 1. **Head-Down Signal Extraction**

**Mathematical Definition:**

Instead of absolute angles, extract relative change:
$$\text{HeadDown} = \max(0, \text{baseline}_{\text{roll}} - \text{current}_{\text{roll}})$$

**Why This Matters:**

```
Scenario 1: Driver naturally tilts head (normal)
baseline_roll = -5Â°
current_roll = -5Â°
head_down = max(0, -5 - (-5)) = 0 âœ… No false alert

Scenario 2: Driver's head droops (drowsy)
baseline_roll = -5Â°
current_roll = -15Â°
head_down = max(0, -5 - (-15)) = 10Â° âœ… Alert triggered

Scenario 3: Driver looks away (but alert)
baseline_roll = -5Â°
current_roll = 10Â°
head_down = max(0, -5 - 10) = 0 âœ… No false alert (looking away != drowsy)
```

**Code:**
```python
head_down_signal = max(0.0, float(baseR) - float(roll))
```

### 2. **OR Logic for Eye Detection**

**Standard (AND Logic):**
```python
if earLeft < 0.11 AND earRight < 0.11:
    drowsy = True
# Problem: Requires BOTH eyes closed (too strict)
```

**Implemented (OR Logic):**
```python
if earLeft < 0.11 OR earRight < 0.11:
    drowsy = True
# Benefit: Either eye closing triggers detection (more sensitive)
```

**Biological Justification:**
- Humans often close one eye first when tired
- Unilateral eye closure is valid drowsiness indicator
- Safer to detect than missing drowsiness

### 3. **Dynamic Window Sizing**

Instead of fixed PERCLOS window (90 frames):

```python
# Adaptive to FPS
fps = cap.get(cv2.CAP_PROP_FPS)
window_frames = int(3.0 * fps)  # Always 3 seconds

# Adapts if:
# 30 FPS â†’ 90 frames
# 60 FPS â†’ 180 frames
# 15 FPS â†’ 45 frames
```

**Benefit:** System works at any frame rate

### 4. **Optional Deep Learning Integration**

**Signal-based Classification:**

Instead of hand-crafted rules, use neural network:

```python
# Input vector
x = [EAR, PERCLOS, Pitch, MAR]

# Model predicts
p_danger = model.predict(x)

# Integrate with rules
if p_danger > 0.85 AND sustained_for_15_frames:
    state = 'DANGER'
```

**Architecture:**
```
Input: [EAR, PERCLOS, Pitch, MAR] (4 features)
  â†“
[Hidden: 32 neurons, ReLU]
  â†“
[Hidden: 16 neurons, ReLU]
  â†“
Output: p(danger) [Sigmoid]
```

**Benefits:**
- Learns non-linear decision boundaries
- Captures complex interactions
- Can adapt to dataset patterns
- Works alongside rules (ensemble)

---

## âš¡ Performance Optimization

### Computational Complexity

**Per-Frame Operations:**

| Operation | Complexity | Time (ms) |
|-----------|-----------|-----------|
| Face mesh detection | O(1) | 30-50 |
| EAR calculation | O(1) | < 1 |
| Head pose (solvePnP) | O(1) | 5-10 |
| Moving avg filter | O(M) | < 1 |
| PERCLOS calc | O(W) | < 1 |
| Decision engine | O(1) | < 1 |
| **Total** | - | **35-65 ms** |

At 30 FPS: 33 ms per frame â†’ System is **real-time capable**

### Memory Usage

```
Signal Buffer (300 samples, 7 signals):
= 300 Ã— 7 Ã— 4 bytes (float32) = 8.4 KB

Smoothed signals:
= 300 Ã— 3 Ã— 4 = 3.6 KB

History (decision engine):
= 500 Ã— 4 = 8 KB

MediaPipe model:
= ~30 MB (cached)

Total: ~50-100 MB
```

### Optimization Techniques

1. **Circular Buffers:** O(1) insertion, no reallocation
2. **Convolution (NumPy):** Vectorized, CPU-optimized
3. **Lazy evaluation:** Only compute when buffer ready
4. **Early exit:** Stop processing if face not detected

---

## ğŸ“‹ Practical Implementation Guide

### Step 1: Initialize Signal Processing

```python
# Create components
signal_buffer = SignalBuffer(max_size=300)
ear_filter = MovingAverageFilter(window_size=7)
pitch_filter = MovingAverageFilter(window_size=15)
mar_filter = MovingAverageFilter(window_size=10)
perclos_calc = PERCLOSCalculator(window_size=90, threshold=0.20)
decision_engine = DrowsinessDecisionEngine()
```

### Step 2: Per-Frame Processing

```python
def process_frame(frame, ear_left, ear_right, pitch, roll, yaw, mar):
    # Add to buffer
    signal_buffer.add_sample(ear_left, ear_right, pitch, roll, yaw, mar)
    
    # Wait for buffer to fill
    if not signal_buffer.is_ready(min_samples=30):
        return 'CALIBRATING', "Establishing baseline..."
    
    # Get and filter signals
    raw_ear = signal_buffer.get_array('ear_avg')
    smoothed_ear = ear_filter.apply(raw_ear)
    current_smoothed_ear = smoothed_ear[-1]
    
    raw_pitch = signal_buffer.get_array('pitch')
    smoothed_pitch = pitch_filter.apply(raw_pitch)
    current_smoothed_pitch = smoothed_pitch[-1]
    
    raw_mar = signal_buffer.get_array('mar')
    smoothed_mar = mar_filter.apply(raw_mar)
    current_smoothed_mar = smoothed_mar[-1]
    
    # Compute PERCLOS
    perclos = perclos_calc.compute(smoothed_ear)
    
    # Make decision
    state, reason = decision_engine.evaluate(
        current_smoothed_ear,
        perclos,
        current_smoothed_pitch,
        current_smoothed_mar
    )
    
    return state, reason
```

### Step 3: Tuning Thresholds

**If Too Many False Alarms:**
```python
# Increase thresholds (less sensitive)
ear_danger_threshold = 0.12  # was 0.11
perclos_danger_threshold = 55.0  # was 50.0
ear_sustained_frames = 120  # was 90 (4 sec instead of 3)
```

**If Missing Real Drowsiness:**
```python
# Decrease thresholds (more sensitive)
ear_danger_threshold = 0.10  # was 0.11
perclos_danger_threshold = 45.0  # was 50.0
ear_sustained_frames = 60  # was 90 (2 sec instead of 3)
```

### Step 4: Logging and Analysis

```python
# Record signals for analysis
signal_log = {
    'timestamp': [],
    'raw_ear': [],
    'smoothed_ear': [],
    'perclos': [],
    'state': [],
    'pitch': []
}

# During processing
signal_log['timestamp'].append(time.time())
signal_log['raw_ear'].append(ear)
signal_log['smoothed_ear'].append(current_smoothed_ear)
signal_log['perclos'].append(perclos)
signal_log['state'].append(state)
signal_log['pitch'].append(current_smoothed_pitch)

# Save for post-processing
import json
with open('signal_log.json', 'w') as f:
    json.dump(signal_log, f)
```

---

## ğŸ“ˆ Signal Analysis Example

### Real-World Scenario: Driver Becoming Drowsy

**Timeline:**

```
Time (sec)  Frame  Raw_EAR  Smooth_EAR  PERCLOS  State   Event
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
0-3         0-90   0.28     0.28        5%       OK      Baseline calibration
3-30        90-900 0.27     0.27        8%       OK      Normal driving

30-45       900-1350 0.25   0.26        10%      OK      Mild fatigue starts
            â†“
45-60       1350-1800 0.20  0.20        25%      OK      Eyes getting tired
            â†“
60-75       1800-2250 0.15  0.15        40%      OK      Significant closure
            â†“
75-90       2250-2700 0.10  0.10        55%      DANGER  âš  THRESHOLD CROSSED
            (90 frames sustained @ 0.10 < 0.11)    (PERCLOS 55% > 50%)
            â†“
90-105      2700-3150 0.09  0.09        62%      DANGER  Beeping!
            â†“
105-120     3150-3600 0.28  0.28        15%      OK      Driver opens eyes
            (Recovery: EAR > 0.16 = 0.11 + 0.05)
```

**Signal Visualization:**

```
EAR
â”‚
0.30â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                             â”‚
0.25â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”
    â”‚                             â”‚  â”‚
0.20â”œâ”€â”€â”                          â”‚  â”‚
    â”‚  â”‚                          â”‚  â””â”€â”¬â”€â”€â”
0.10â”œâ”€â”€â”´â”€â”€â”€â”€â”€â”€â”                  â”‚    â””â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚                  â”‚            â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€
0.00â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    0        30        60       90       120
              â†‘                â†‘         â†‘
         Start tired     DANGER state  Recovery
```

**Decision Timeline:**

```
t=0-90s:   State = OK
           Reason: "Calibrating... (90/90 samples)"
           
t=90-240s: State = OK
           PERCLOS increases 5% â†’ 55%
           But EAR not sustained < 0.11 yet
           Reason: "Normal (EAR: 0.27, PERCLOS: 8%, ...)"
           
t=240-270s: State = DANGER
            Reason: "High eye closure (55% > 50%)"
            PERCLOS > 50% for 90 frames
            
t=270-330s: State = DANGER
            Still sustained
            Reason: "Eyes closed (0.09 < 0.11)"
            Audio: BEEP BEEP BEEP...
            
t=330-360s: State = OK
            EAR > 0.16 (hysteresis margin)
            Reason: "âœ“ Driver alert - All metrics normal"
            Audio: STOP
```

---

## ğŸ¯ Best Practices for Signal Processing

1. **Always Calibrate:** First 3 seconds establish baseline
2. **Use Hysteresis:** Prevents state flickering
3. **Monitor Multiple Signals:** No single signal is definitive
4. **Log Data:** Enables post-analysis and improvement
5. **Test Edge Cases:** Low light, glasses, different faces
6. **Tune Thresholds:** Match specific deployment environment
7. **Handle Failures:** Missing faces, poor detection gracefully
8. **Validate with Real Data:** Run on real drivers, not synthetic

---

## ğŸ“š References & Research

### Key Publications

1. **EAR (Eye Aspect Ratio)**
   - SoukupovÃ¡, T., & ÄŒech, J. (2016)
   - "Real-Time Eye Blink Detection using Facial Landmarks"
   - IEEE Transactions on Pattern Analysis and Machine Intelligence

2. **PERCLOS**
   - Wierwille, W. W., et al. (1994)
   - "Research on vehicle-based driver status/performance monitoring"
   - NHTSA Report

3. **Head Pose Estimation**
   - Murphy-Chutorian, E., & Trivedi, M. M. (2009)
   - "Head Pose Estimation in Computer Vision: A Survey"
   - IEEE Transactions on Pattern Analysis and Machine Intelligence

4. **Signal Processing**
   - Oppenheim, A. V., & Schafer, R. W. (2009)
   - "Discrete-Time Signal Processing"
   - Prentice Hall (foundational textbook)

---

**Document Version:** 1.0  
**Last Updated:** December 26, 2025  
**Focus:** Signal Processing & Mathematical Formulas

